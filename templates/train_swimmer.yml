# DRL Algo config
# drl_config: 
#     environment:
#         env: SwimmerSurrogate
#         env_config:
#             dyn_model: null # None
#             max_episode_steps: 1000
#             mod_angles: True
#             reset_on_bounds: True
#     framework: 
#         framework: torch
#     evaluation:
#         evaluation_config:
#             env_config:
#                 dyn_model: null # None
#                 max_episode_steps: 1000
#                 mod_angles: True
#                 reset_on_bounds: True
#             explore: False
#         evaluation_interval: 1

drl_class: 'PPO'
baseline: False
train-iterations: 3000 #3000

drl_config: 
    env: SwimmerSurrogate
    env_config:
        dyn_model: null # None
        max_episode_steps: 1000
        mod_angles: True
        reset_on_bounds: True
 
    framework: torch
    evaluation_config:
        env_config:
            dyn_model: null # None
            max_episode_steps: 1000
            mod_angles: False
            reset_on_bounds: False
        explore: False
    evaluation_interval: 1
    evaluation_duration: 10


# Ray Configs
ray_config:
    run_config:
        stop:
            # timesteps_total
            num_env_steps_sampled: 1.2e+6 
    tune_config:
        num_samples: 20 # number of sample trials to run
    checkpoint_freq: 10


# SINDy Configs
init_collection:
    collect_seed: 0 
    n_random_steps: 4000
    n_null_steps: 8000
    max_traj_len: 100

sindy_fit:
    fit_freq: 10 # how many training iterations to perform before fitting

dyn_model_config:
    affine_config: 
        poly_deg: 2 
        n_state: 8 
        n_control: 2
        poly_int: False
        tensor: True
    base_config:
        threshold: 0.02 
        alpha: 0.5
    ensemble_config:
        bagging: True 
        library_ensemble: True
        n_models: 100