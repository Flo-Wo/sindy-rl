drl_class: 'PPO'
baseline: False
train-iterations: 3000 #3000

drl_config: 
    env: SurrogateCylinder
    env_config:
        dyn_model: null # None
        control_freq: 50   
        n_skip_on_reset: 50
        max_episode_steps: 200 
        use_omega: True
        use_CL_dot: True
        hydro_config:
            flow_config:
                mesh: coarse
                restart: '/home/firedrake/sindy-rl/data/hydro/cylinder/2023-02-11_ctrl=50_t=5.5s.ckpt'
 
    framework: torch
    evaluation_config:
        env_config:
            dyn_model: null # None
            control_freq: 50   
            n_skip_on_reset: 50
            max_episode_steps: 200
            use_omega: True
            use_CL_dot: True
            hydro_config:
                flow_config:
                    mesh: coarse
                    restart: '/home/firedrake/sindy-rl/data/hydro/cylinder/2023-02-11_ctrl=50_t=5.5s.ckpt'
 
        explore: False
    evaluation_interval: 50
    evaluation_duration: 1


# Ray Configs
ray_config:
    run_config:
        stop:
            # timesteps_total
            num_env_steps_sampled: 1.2e+6
        log_to_file: True 
    tune_config:
        num_samples: 20 # number of sample trials to run
    checkpoint_freq: 50


# SINDy Configs
init_collection:
    collect_seed: 0 
    n_random_steps: 250
    n_null_steps: 250
    max_traj_len: 100
    # load_path: '/home/firedrake/sindy-rl/data/hydro/cylinder/test_rand_data_50freq_50skip_2023-02-08.pkl'

sindy_fit:
    n_pi_collect: 100 # how many steps to collect when on-policy
    fit_freq: 25 # how many training iterations to perform before fitting

dyn_model_config:
    affine_config: 
        poly_deg: 2
        n_state: 4 
        n_control: 1
        poly_int: True
        tensor: False
    base_config:
        threshold: 0.001 
        alpha: 0.00005
    ensemble_config:
        bagging: True 
        library_ensemble: True
        n_models: 100