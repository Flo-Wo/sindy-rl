n_train_iter: 40000
dyn_fit_freq: 20
exp_dir: tmp2
fcnet_hiddens: [64, 64]

# used for rolling out new on-policy data for 
# dynamics fitting
real_env:
  class: DMCEnvWrapper
  config: 
      domain_name: "cartpole"
      task_name: "swingup"
      frame_skip: 1
      from_pixels: False
      task_kwargs:

drl:
  class: PPO
  config:
    lr_schedule: [[0, 3.0e-4], [10000000, 3.0e-9]]
    # rollout_fragment_length: 2048
    # sgd_minibatch_size: 64
    # num_sgd_iter: 10
    lambda: 0.95
    vf_loss_coeff: 0.5
    vf_clip_param: 0.2
    clip_param: 0.2
    grad_clip: 0.5
    env: 
    env_config:
      max_episode_steps: 1000
      real_env_class: DMCEnvWrapper
      real_env_config: 
        domain_name: "cartpole"
        task_name: "swingup"
        frame_skip: 1
        from_pixels: False
        task_kwargs:
      init_real_on_start: True
      use_real_env: False
      ensemble_modes: 
        dyn: sample #median
        rew: # median
      init_weights: True
      act_dim: 1
      obs_dim: 5
      act_bounds: 
        - [-1, 1]
      obs_bounds: 
        - [-5, 5] # pos
        - [-1.1, 1.1] # cos(th)
        - [-1.1, 1.1] # sin(th)
        - [-10, 10] # dx
        - [-10, 10] # dth

    framework: torch

    evaluation_config:
      env_config:
        init_real_on_start: True
        use_real_env: True
        use_bounds: False
        real_env_class: DMCEnvWrapper
        real_env_config: 
          domain_name: "cartpole"
          task_name: "swingup"
          from_pixels: False
          task_kwargs:
      explore: False
    evaluation_interval: 1
    evaluation_duration: 5


off_policy_buffer:
  config:
    max_traj:
    max_samples:
  init: 
    type: collect
    kwargs: 
      n_steps: 4000 # 12000
      n_steps_reset: 1000
      # seed: 0 # TO-DO: set seed in dmc env

on_policy_buffer:
  config:
    max_traj: 
    max_samples: 4000 # 12000
  collect:
    n_steps: 1000
    n_steps_reset: 2000

dynamics_model:
  class: EnsembleSINDyDynamicsModel
  config:
    'callbacks': project_cartpole
    'dt': 1
    'discrete': True 
    'optimizer': 
      'base_optimizer':
        'name': 'STLSQ'
        'kwargs': 
          'alpha': 5.0e-5
          'threshold': 7.0e-3
      'ensemble':
        'bagging': True
        'library_ensemble': True
        'n_models': 20
    'feature_library': 
      name: affine
      kwargs:
        poly_deg: 2
        n_state: 5 
        n_control: 1
        poly_int: True
        tensor: True


rew_model:
  class: FunctionalRewardModel
  config: 
    name: cart_reward
    
# rew_model:
#   class: EnsembleSparseRewardModel
#   config:
#     'use_control': True
#     'optimizer': 
#       'base_optimizer':
#         'name': 'STLSQ'
#         'kwargs': 
#           'alpha': 1.0e-5
#           'threshold': 5.0e-2
#       'ensemble':
#         'bagging': True
#         'library_ensemble': True
#         'n_models': 100
#     'feature_library': 
#       name: PolynomialLibrary
#       kwargs:
#         degree: 2
#         include_bias: False
#         include_interaction: True

ray_config:
    run_config:
        name: "2023-09-09_test" #"dyna_dmc-cartpole_freq=20_len=1k_coll=4k_4k_rew=fixed_ensemble=sample-20_cleanRL-64"
        stop:
            # timesteps_total
            num_env_steps_sampled: 1.0e+7
        log_to_file: True 
    tune_config:
        num_samples: 20 # number of sample trials to run
    checkpoint_freq: 50