exp_dir: cart-swingup
n_train_iter: 40000
dyn_fit_freq: 10
fcnet_hiddens: [64, 64]
# use_pbt: True

# used for rolling out new on-policy data for 
# dynamics fitting
real_env:
  class: DMCEnvWrapper
  config: 
      domain_name: "cartpole"
      task_name: "swingup"
      frame_skip: 1
      from_pixels: False
      task_kwargs:
        time_limit: 10 # DEFAULT IS 10!

drl:
  class: PPO
  config:
    training: 
      lr_schedule: [[0, 3.0e-4], [10000000, 3.0e-9]]
      gamma: 0.99
      # lr: 3.0e-4
      lambda_: 0.95
      vf_loss_coeff: 0.5
      vf_clip_param: 0.2
      clip_param: 0.2
      grad_clip: 0.5
    environment:
      env: 
      env_config:
        max_episode_steps: 1000
        real_env_class: DMCEnvWrapper
        real_env_config: 
          domain_name: "cartpole"
          task_name: "swingup"
          frame_skip: 1
          from_pixels: False
          task_kwargs:
            time_limit: 10 # DEFAULT IS 10!
        init_real_on_start: True
        use_real_env: False
        ensemble_modes: 
          dyn: median #median
          rew: # median
        init_weights: True
        act_dim: 1
        obs_dim: 5
        act_bounds: 
          - [-1, 1]
        obs_bounds: 
          - [-5, 5] # pos
          - [-1.1, 1.1] # cos(th)
          - [-1.1, 1.1] # sin(th)
          - [-10, 10] # dx
          - [-10, 10] # dth
    framework: torch
    evaluation: 
      evaluation_config:
        env_config:
          max_episode_steps: 1000
          init_real_on_start: True
          use_real_env: True
          use_bounds: False
          real_env_class: DMCEnvWrapper
          real_env_config: 
            domain_name: "cartpole"
            task_name: "swingup"
            from_pixels: False
            task_kwargs:
              time_limit: 10 # DEFAULT IS 10!
        explore: False 
      evaluation_interval: 1
      evaluation_duration: 5
      evaluation_duration_unit: "episodes"
      always_attach_evaluation_results: True


off_policy_buffer:
  config:
    max_traj:
    max_samples:
  init: 
    type: collect
    kwargs: 
      n_steps: 8000 # 12000
      n_steps_reset: 1000
      # seed: 0 # TO-DO: set seed in dmc env

on_policy_buffer:
  config:
    max_traj: 
    max_samples: 8000 # 12000
  collect:
    n_steps: 500
    n_steps_reset: 2000

dynamics_model:
  class: EnsembleSINDyDynamicsModel
  config:
    'callbacks': project_cartpole
    'dt': 1
    'discrete': True 
    'optimizer': 
      'base_optimizer':
        'name': 'STLSQ'
        'kwargs': 
          'alpha': 5.0e-5
          'threshold': 7.0e-3
      'ensemble':
        'bagging': True
        'library_ensemble': True
        'n_models': 20
    'feature_library': 
      name: affine
      kwargs:
        poly_deg: 2
        n_state: 5 
        n_control: 1
        poly_int: True
        tensor: True


rew_model:
  class: FunctionalRewardModel
  config: 
    name: cart_reward
    
# rew_model:
#   class: EnsembleSparseRewardModel
#   config:
#     'use_control': True
#     'optimizer': 
#       'base_optimizer':
#         'name': 'STLSQ'
#         'kwargs': 
#           'alpha': 1.0e-5
#           'threshold': 5.0e-2
#       'ensemble':
#         'bagging': True
#         'library_ensemble': True
#         'n_models': 100
#     'feature_library': 
#       name: PolynomialLibrary
#       kwargs:
#         degree: 2
#         include_bias: False
#         include_interaction: True

ray_config:
    run_config:
        name: "dyna-sindy_freq=10_coll=500_buff=8k_8k_ens-rew=fixed_ens-dyn=med-20_steps"
        stop:
            # timesteps_total
            num_env_steps_sampled: 1.0e+7
        log_to_file: True 
    tune_config:
        num_samples: 20 # number of sample trials to run
    checkpoint_freq: 10

# pbt_config:
#   mode: "max"
#   # metric: "evaluation/episode_reward_mean"
#   # metric: "traj_buffer/on_policy_ep_mean_rew"
#   metric: "dyn_collect/mean_rew"
#   time_attr: "training_iteration"
#   perturbation_interval: 50
#   resample_probability: 0.25
#   quantile_fraction: 0.25
#   synch: True

#   hyperparam_mutations:
#     drl/config/training/lr: 
#       search_class: choice
#       search_space: [[1.0e-6, 5.0e-6, 1.0e-5, 5.0e-5, 1.0e-4, 5.0e-4, 1.0e-3, 5.0e-3]]
#     drl/config/training/lambda_: 
#       search_class: choice
#       search_space: [[0.9, 0.95, 0.99, 0.999, 0.9999, 1.0]]
#     drl/config/training/gamma: 
#       search_class: choice
#       search_space: [[0.9, 0.95, 0.99, 0.999, 0.9999, 1.0]]