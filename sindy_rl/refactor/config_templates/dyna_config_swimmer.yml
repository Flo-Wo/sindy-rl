n_train_iter: 40000
dyn_fit_freq: 5
exp_dir: tmp2
fcnet_hiddens: [256, 256]

# used for rolling out new on-policy data for 
# dynamics fitting
real_env:
  class: SwimmerWithBoundsClassic
  config: 
    env_kwargs: {}
    max_episode_steps: 1000
    reward_threshold: 360
    reset_on_bounds: True
    # noise: [0.10, 0.18, 0.18, 0.065, 0.20, 0.20, 0.41, 0.41]

drl:
  class: PPO
  config:
    lr_schedule: [[0, 3.0e-4], [10000000, 3.0e-9]]
    # rollout_fragment_length: 2048
    # sgd_minibatch_size: 64
    # num_sgd_iter: 10
    lambda: 0.95
    vf_loss_coeff: 0.5
    vf_clip_param: 0.2
    clip_param: 0.2
    grad_clip: 0.5
    env: 
    env_config:
      real_env_class: SwimmerWithBoundsClassic
      real_env_config: 
        reset_on_bounds: True
        # noise: [0.10, 0.18, 0.18, 0.065, 0.20, 0.20, 0.41, 0.41]
      init_real_on_start: True
      ensemble_modes: 
        dyn: median
        rew: # median
      init_weights: True
      act_dim: 2
      obs_dim: 8
      act_bounds: 
        - [-1, 1]
        - [-1, 1]
      obs_bounds: 
        - [-3.1415, 3.1415]
        - [-1.7453, 1.7453]
        - [-1.7453, 1.7453]
        - [-10, 10]
        - [-10, 10]
        - [-10, 10]
        - [-10, 10]
        - [-10, 10]

    framework: torch

    evaluation_config:
      env_config:
        init_real_on_start: True
        use_real_env: True
        use_bounds: False
        real_env_class: SwimmerWithBoundsClassic
        real_env_config: 
          # noise: [0.10, 0.18, 0.18, 0.065, 0.20, 0.20, 0.41, 0.41]
          reset_on_bounds: False
          max_episode_steps: 1000
          reward_threshold: 360
      explore: False
    evaluation_interval: 1
    evaluation_duration: 5


off_policy_buffer:
  config:
    max_traj:
    max_samples:
  init: 
    type: collect
    kwargs: 
      n_steps: 4000
      n_steps_reset: 100
      seed: 0

on_policy_buffer:
  config:
    max_traj: 
    max_samples: 4000
  collect:
    n_steps: 1000
    n_steps_reset: 2000

dynamics_model:
  class: EnsembleSINDyDynamicsModel
  config:
    'dt': 1
    'discrete': True 
    'optimizer': 
      'base_optimizer':
        'name': 'STLSQ'
        'kwargs': 
          'alpha': 5.0e-1
          'threshold': 2.0e-2
      'ensemble':
        'bagging': True
        'library_ensemble': True
        'n_models': 20
    'feature_library': 
      name: affine
      kwargs:
        poly_deg: 2
        n_state: 8 
        n_control: 2
        poly_int: False
        tensor: True
      # name: PolynomialLibrary
      # kwargs:
      #   degree: 3
      #   include_bias: False
      #   include_interaction: False

rew_model:
  class: FunctionalRewardModel
  config: 
    name: swimmer_reward
# rew_model:
#   class: EnsembleSparseRewardModel
#   config:
#     'use_control': False
#     'optimizer': 
#       'base_optimizer':
#         'name': 'STLSQ'
#         'kwargs': 
#           'alpha': 1.0e-5
#           'threshold': 5.0e-2
#       'ensemble':
#         'bagging': True
#         'library_ensemble': True
#         'n_models': 20
#     'feature_library': 
#       name: PolynomialLibrary
#       kwargs:
#         degree: 2
#         include_bias: False
#         include_interaction: False

ray_config:
    run_config:
        name: "dyna_swimmer_affine_freq=5_len=1k_coll=4k_4k_rew=fixed_ensemble=med-20_cleanRL-256"
        stop:
            # timesteps_total
            num_env_steps_sampled: 1.0e+7
        log_to_file: True 
    tune_config:
        num_samples: 20 # number of sample trials to run
    checkpoint_freq: 50